{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp kolsOnS2AG\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Key Opinion Leader Analysis\n",
    "\n",
    "> API Details here\n",
    "\n",
    "System to try to unpack key opinion leaders + close associates / prominent researchers based on co-authorship and co-citation within the same network of papers. \n",
    "\n",
    "1. List key opinion leaders (KOL)\n",
    "2. Disambiguate each KOL based on clustering within bioinformatics + machine learning field\n",
    "3. List co-authors + referenced authors + citing authors\n",
    "4. Build paper / author networks \n",
    "5. Derive co-citation graphs of authors\n",
    "6. Perform Author-based Eigenfactor on citation networks to list most influential authors\n",
    "7. Provide links to the KOL that generated the link to each influential author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import os.path\n",
    "from urllib.parse import quote_plus\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import pickle \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy import linalg\n",
    "\n",
    "class KOLsOnS2AG:\n",
    "  \"\"\"This class permits the construction of a local NetworkX graph that copies the basic organization of S2AG data.<BR>\n",
    "  Functionality includes:\n",
    "    * query all papers, references, and cited papers of a single individual\n",
    "    * build an author-to-author citation graph\n",
    "    * run eigenfactor analysis over the author graph\n",
    "  \n",
    "  Attributes:\n",
    "    * x_api_key: an API key obtained from Semantic Scholar (https://www.semanticscholar.org/product/api)\n",
    "    * author_stem_url, paper_stem_url: urls for API endpoints in S2AG\n",
    "    * g: the networkx graph representing the citation / authorship network \n",
    "    * added_papers: papers that have had all citations and references added to graph\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, x_api_key):\n",
    "    \"\"\" Initialize the interface with an API key. \n",
    "    \"\"\"\n",
    "    self.author_search_url = 'https://api.semanticscholar.org/graph/v1/author/search'\n",
    "    self.author_stem_url = 'https://api.semanticscholar.org/graph/v1/author/'\n",
    "    self.paper_stem_url = 'https://api.semanticscholar.org/graph/v1/paper/'\n",
    "    self.x_api_key = x_api_key\n",
    "    self.added_papers = set()\n",
    "    self.g = nx.DiGraph()\n",
    "\n",
    "  def print_basic_stats(self):\n",
    "    \"\"\" Prints out basic stats about the current graph stored in memory.\n",
    "    \"\"\"\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(\"# Papers: %d\"%(len(self.search_nodes('paper'))))\n",
    "    print(\"# Authors: %d\"%(len(self.search_nodes('author'))))\n",
    "    print(\"# Authorship: %d\"%(len(self.search_edges('wrote'))))\n",
    "    print(\"# Citations: %d\"%(len(self.search_edges('cites'))))\n",
    "    infCites = [(e1, e2) for e1,e2,attrs in self.search_edges('cites') if attrs.get('isInfluential')]\n",
    "    print(\"# Influential Citations: %d\"%(len(infCites)))\n",
    "    print(\"SCC: %d\"%(nx.number_strongly_connected_components(self.g)))\n",
    "    print(\"WCC: %d\"%(nx.number_weakly_connected_components(self.g)))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "  # ~~~~~~~~~~ HIGH-LEVEL API ~~~~~~~~~~ \n",
    "  def search_for_disambiguated_author(self, author_name):\n",
    "    \"\"\" Searches for an author and returns 10 most influential papers for each disambiguated example. \n",
    "    \"\"\"\n",
    "    search_url = self.author_search_url+'?query='+author_name+'&fields=name,paperCount,hIndex,papers.paperId,papers.title,papers.influentialCitationCount'\n",
    "    r = requests.get(search_url, headers={\"x-api-key\":self.x_api_key})   \n",
    "    author_data = json.loads(r.content.decode(r.encoding))\n",
    "    n_found = author_data.get('total')\n",
    "    df2 = pd.DataFrame(author_data.get('data'))\n",
    "    #df2 = df2[df2.paperCount > n_papers_threshold]\n",
    "    paper_titles = []\n",
    "    for row2 in df2.itertuples():\n",
    "      p_text_list = '     |     '.join([p.get('title') for p in sorted(row2.papers, key=lambda d: d['influentialCitationCount'], reverse=True)[:10]])\n",
    "      paper_titles.append(p_text_list)\n",
    "      #print(json.dumps(p, indent=4, sort_keys=True))  \n",
    "      #  paperIds.add(p.get('paperId'))\n",
    "    #df2 = df2.drop(columns=['papers'])\n",
    "    df2['Top 10 Pubs'] = paper_titles\n",
    "    return df2\n",
    "\n",
    "  def build_author_citation_graph(self, authorId, pkl_file=None):\n",
    "    \"\"\"This builds a complete graph for a given individual based on \n",
    "    their papers, and 'highly influential' references + citations of those papers.\n",
    "    See https://www.nature.com/articles/547032a. The system will then fill in \n",
    "    all cross-references of papers in this graph that cite each other. \n",
    "    \"\"\"\n",
    "    # Build the S2 author / paper graph\n",
    "    self.addKeyOpinionLeader(authorId)\n",
    "    inf_edges = [(n1,n2) for n1,n2,attrs in self.search_edges('cites') if attrs.get('isInfluential')]\n",
    "    inf_papers = set([n1 for n1,n2 in inf_edges]).union(set([n2 for n1,n2 in inf_edges]))\n",
    "    g2 = self.get_influential_graph()\n",
    "    self.g = g2\n",
    "    self.addCitationsOrReferencesToGraph(inf_papers, 'references', True, pkl_file)\n",
    "    for p in inf_papers:\n",
    "      self.added_papers.add(p)\n",
    "  \n",
    "  def run_thresholded_centrality_analysis(self, min_pub_count=3, top_n=100 ):\n",
    "    \"\"\"The system will analyse all authors within the graph that have total \n",
    "    number of publications above `min_pub_count` by peforming an author-based\n",
    "    eigenfactor calculation (see [West et al 2013](https://jevinwest.org/papers/West2013JASIST.pdf)) \n",
    "    and then return a pandas data fram of the `top_n` most central authors \n",
    "    in the graph.\n",
    "    \"\"\"\n",
    "    thresholded_authors, counts  = self.threshold_authors_by_pubcount(min_pub_count)\n",
    "    author_eigfacs_df = self.compute_author_eigenfactors(thresholded_authors, verbose=True)\n",
    "    top_n_df = author_eigfacs_df.sort_values('f',ascending=False)[0:top_n]\n",
    "    authorIds = [row.id for row in top_n_df.itertuples()]\n",
    "    topn_author_metadata_df = self.query_authors_metadata(authorIds)\n",
    "    return topn_author_metadata_df.set_index('authorId').join(top_n_df.set_index('id'))\n",
    "    \n",
    "  # ~~~~~~~~~~ BUILDING THE GRAPH FROM S2AG ~~~~~~~~~~    \n",
    "  def executeSemScholAuthorPapersQueryWithOffset(self, authorId, offset, verbose=True):\n",
    "    fields = [\n",
    "        'paperId',\n",
    "        'authors',\n",
    "        'referenceCount'\n",
    "      ]\n",
    "    url = '%s%d/papers?fields=%s&limit=1000&offset=%d'%(self.author_stem_url,authorId,','.join(fields),offset)\n",
    "    \n",
    "    if verbose:\n",
    "      print('AUTHOR_ID: %d'%(authorId))\n",
    "      print(url)\n",
    "    \n",
    "    r = requests.get(url, headers={\"x-api-key\":self.x_api_key}, timeout=20)   \n",
    "    author_response = json.loads(r.content.decode(r.encoding))\n",
    "    rdata = author_response.get('data')\n",
    "    \n",
    "    if rdata is None:\n",
    "      return []\n",
    "    \n",
    "    #print(json.dumps(rdata, indent=4, sort_keys=True))\n",
    "\n",
    "    paperTuples = list(set([(p_hash.get('paperId'), \n",
    "                             len(p_hash.get('authors')), \n",
    "                             p_hash.get('referenceCount'))\n",
    "                         for p_hash in rdata if p_hash.get('referenceCount') is not None]))\n",
    "    if verbose:\n",
    "      print('Adding papers:'+str(len(paperTuples)))\n",
    "\n",
    "    authorIds = list(set([a_hash.get('authorId') \n",
    "                          for p_hash in rdata \n",
    "                          for a_hash in p_hash.get('authors')\n",
    "                          if a_hash.get('authorId') is not None]))\n",
    "    if verbose:\n",
    "      print('Adding authors:'+str(len(authorIds)))\n",
    "    \n",
    "    authorEdges1 = list(set([(a_hash.get('authorId'),p_hash.get('paperId')) \n",
    "                             for p_hash in rdata \n",
    "                             for a_hash in p_hash.get('authors')\n",
    "                             if a_hash.get('authorId') is not None]))\n",
    "    authorEdges2 = [(e2,e1) for (e1,e2) in authorEdges1]\n",
    "    if verbose:\n",
    "      print('Adding author edges:'+str(len(authorEdges1)))\n",
    "\n",
    "    for tup in paperTuples:\n",
    "      self.g.add_node(tup[0], label='paper', nAuthors=int(tup[1]), nRefs=int(tup[2]))\n",
    "    self.g.add_nodes_from(authorIds, label='author' )\n",
    "    self.g.add_edges_from(authorEdges1, label='wrote')\n",
    "    self.g.add_edges_from(authorEdges2, label='was_written_by')\n",
    "    \n",
    "    return rdata\n",
    "\n",
    "  # structure of data \n",
    "  # kolId = [{paperId,\n",
    "  #                authors:[{authorId,name}], \n",
    "  #                citations:[{paperId,authors:[{authorId,name}]}], \n",
    "  #                references:[{paperId,authors:[{authorId,name}]}]}]\n",
    "  def runSemScholAuthorPapersQuery(self, authorId, verbose=False):\n",
    "    offset = 0\n",
    "    rdata = []\n",
    "    while len(rdata)%1000 == 0: \n",
    "      rdata = self.executeSemScholAuthorPapersQueryWithOffset(authorId, offset, verbose)\n",
    "      offset += 1000\n",
    "  \n",
    "  def addKeyOpinionLeader(self, kolId, pkl_file=None, verbose=False):\n",
    "    '''Given an author `kol`, add all papers published by `kol` to `g`. \n",
    "    Then, add all citations and references of those papers to `g`, \n",
    "    and add them to `added papers`.\n",
    "    '''\n",
    "    self.runSemScholAuthorPapersQuery(kolId, verbose=verbose)\n",
    "    kol_papers = [e2 for e1, e2, attrs in self.g.out_edges(str(kolId), data=True) if attrs.get('label') == 'wrote']\n",
    "    self.addCitationsOrReferencesToGraph(kol_papers, 'citations', False, pkl_file)\n",
    "    self.addCitationsOrReferencesToGraph(kol_papers, 'references', False, pkl_file)\n",
    "    for p in kol_papers:\n",
    "      self.added_papers.add(p)\n",
    "    \n",
    "  def addCitationsOrReferencesWithOffset(self, paperId, citref, offset, isClosed, verbose=False):\n",
    "    if citref == 'citations':\n",
    "      citing_cited = 'citingPaper'\n",
    "    elif citref == 'references':\n",
    "      citing_cited = 'citedPaper' \n",
    "    else:\n",
    "      raise Exception('error with citref: '+citref)\n",
    "\n",
    "    paper_stem_url = 'https://api.semanticscholar.org/graph/v1/paper/'\n",
    "    fields = [\n",
    "        'paperId',\n",
    "        'authors',\n",
    "        'isInfluential',\n",
    "        'referenceCount',\n",
    "        'year'\n",
    "      ]\n",
    "    url = '%s%s/%s?fields=%s&limit=1000&offset=%d'%(paper_stem_url, paperId, citref, ','.join(fields), offset)\n",
    "    r = requests.get(url, headers={\"x-api-key\":self.x_api_key}, timeout=20)   \n",
    "    paper_response = json.loads(r.content.decode(r.encoding))\n",
    "    rdata = paper_response.get('data')\n",
    "    \n",
    "    if verbose:\n",
    "      print('\\n'+str(paperId))\n",
    "      print(url)\n",
    "    #print(json.dumps(rdata, indent=4, sort_keys=True))\n",
    "  \n",
    "    try:\n",
    "      \n",
    "      paperTuples = list(set([(p_hash.get(citing_cited).get('paperId'), \n",
    "                                len(p_hash.get(citing_cited).get('authors')), \n",
    "                                p_hash.get(citing_cited).get('referenceCount'),\n",
    "                                p_hash.get(citing_cited).get('year'))\n",
    "                           for p_hash in rdata\n",
    "                           if p_hash.get(citing_cited).get('paperId') is not None \n",
    "                              and p_hash.get(citing_cited).get('referenceCount') is not None]))\n",
    "      if verbose:\n",
    "        print('Adding papers:'+str(len(paperTuples)))\n",
    "            \n",
    "      authorIds = list(set([a_hash.get('authorId') \n",
    "                            for p_hash in rdata \n",
    "                            for a_hash in p_hash.get(citing_cited).get('authors')\n",
    "                            if a_hash.get('authorId') is not None]))\n",
    "      if verbose:\n",
    "        print('Adding authors:'+str(len(authorIds)))\n",
    "      \n",
    "      authorEdges1 = list(set([(a_hash.get('authorId'), p_hash.get(citing_cited).get('paperId')) \n",
    "                               for p_hash in rdata \n",
    "                               for a_hash in p_hash.get(citing_cited).get('authors')\n",
    "                               if a_hash.get('authorId') is not None]))\n",
    "      authorEdges2 = [(e2,e1) for (e1,e2) in authorEdges1]\n",
    "      if verbose:\n",
    "        print('Adding author edges:'+str(len(authorEdges1)))\n",
    "            \n",
    "      if citref == 'citations':\n",
    "        citEdges = list(set([(p_hash.get(citing_cited).get('paperId'), paperId, p_hash.get('isInfluential')) \n",
    "                                    for p_hash in rdata \n",
    "                                    if p_hash.get(citing_cited).get('paperId') is not None]))\n",
    "      else: \n",
    "        citEdges = list(set([(paperId, p_hash.get(citing_cited).get('paperId'), p_hash.get('isInfluential')) \n",
    "                                    for p_hash in rdata \n",
    "                                    if p_hash.get(citing_cited).get('paperId') is not None]))\n",
    "        \n",
    "    except:\n",
    "      print('Error in adding citations from paper: '+paperId)\n",
    "      return []\n",
    "\n",
    "    if verbose:\n",
    "      print('Adding citations:'+str(len(citEdges))+'\\n')\n",
    "    \n",
    "    if isClosed:\n",
    "      checked_edge_list = [(e1,e2,sig) for e1,e2,sig in citEdges if e1 in self.g.nodes and e2 in self.g.nodes]\n",
    "      for e1, e2, isInf in checked_edge_list:\n",
    "        self.g.add_edge(e1, e2, label='cites', isInfluential=isInf)        \n",
    "    else:\n",
    "      for tup in paperTuples:\n",
    "        self.g.add_node(tup[0], label='paper', nAuthors=int(tup[1]), nRefs=int(tup[2]), year=tup[3])\n",
    "        #print(tup)\n",
    "      self.g.add_nodes_from(authorIds, label='author' )\n",
    "      self.g.add_edges_from(authorEdges1, label='wrote')\n",
    "      self.g.add_edges_from(authorEdges2, label='was_written_by')\n",
    "      for e1, e2, isInf in citEdges:\n",
    "        self.g.add_edge(e1, e2, label='cites', isInfluential=isInf)        \n",
    "    return rdata\n",
    "  \n",
    "  def addCitationsOrReferencesToGraph(self, paperIds, citref, isClosed, pklpath=None):\n",
    "    \n",
    "    for i, paperId in tqdm(enumerate(paperIds), total=len(paperIds)):\n",
    "      \n",
    "      if pklpath and i%1000==0: # checkpoint save\n",
    "        with open(pklpath, 'wb') as f:\n",
    "          pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)    \n",
    "      \n",
    "      if paperId in self.added_papers:\n",
    "        continue\n",
    "      \n",
    "      offset = 0\n",
    "      rdata = []\n",
    "      while len(rdata)%1000 == 0: \n",
    "        rdata = self.addCitationsOrReferencesWithOffset(paperId, citref, offset, isClosed)\n",
    "        if len(rdata) == 0:\n",
    "          break      \n",
    "        offset += 1000 \n",
    "        \n",
    "    # Final save\n",
    "    if pklpath:\n",
    "      with open(pklpath, 'wb') as f:\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "  # ~~~~~~~~~~ S2 METADATA QUERIES ~~~~~~~~~~  \n",
    "        \n",
    "  def executeSemScholAuthorQuery(self, authorId):\n",
    "    fields = [\n",
    "      'authorId',\n",
    "      'name',\n",
    "      'paperCount',\n",
    "      'citationCount',\n",
    "      'hIndex',\n",
    "      'papers.paperId',\n",
    "      'papers.title',\n",
    "      'papers.influentialCitationCount'\n",
    "      ]\n",
    "    url = '%s%d?fields=%s'%(self.author_stem_url, authorId,','.join(fields))\n",
    "    #print(url)\n",
    "    r = requests.get(url, headers={\"x-api-key\":self.x_api_key}, timeout=20)   \n",
    "    author_response = json.loads(r.content.decode(r.encoding))\n",
    "\n",
    "    p_text_list = '     |     '.join([p.get('title') for p in sorted(author_response['papers'], key=lambda d: d['influentialCitationCount'], reverse=True)[:10]])\n",
    "    author_response['Top_10_Pubs'] = p_text_list\n",
    "    author_response['authorId'] = int(author_response['authorId'])\n",
    "    author_response.pop('papers')\n",
    "    #print(json.dumps(author_response, indent=4, sort_keys=True))\n",
    "\n",
    "    return author_response\n",
    "\n",
    "  def query_authors_metadata(self, authorIds):\n",
    "    extras = []\n",
    "    for authorId in tqdm(authorIds):\n",
    "      rdata = self.executeSemScholAuthorQuery(int(authorId))\n",
    "      extras.append(rdata)\n",
    "    extras_df = pd.DataFrame(extras)\n",
    "    extras_df.set_index('authorId')\n",
    "    return extras_df\n",
    "  \n",
    "  # ~~~~~~~~~~ INFLUENTIAL GRAPH FUNCTIONS ~~~~~~~~~~  \n",
    "\n",
    "  def get_influential_graph(self):\n",
    "    g = self.g\n",
    "    g2 = nx.DiGraph()\n",
    "    inf_edges = [(n1,n2) for n1,n2,attrs in self.search_edges('cites') if attrs.get('isInfluential')]\n",
    "    inf_papers =  set([n1 for n1,n2 in inf_edges]).union(set([n2 for n1,n2 in inf_edges]))\n",
    "    for paperId in tqdm(sorted(list(inf_papers))):\n",
    "      nAuthors = g.nodes[paperId].get('nAuthors')\n",
    "      nRefs = g.nodes[paperId].get('nRefs')\n",
    "      g2.add_node(paperId, label='paper', nAuthors=nAuthors, nRefs=nRefs)        \n",
    "      for e1, e2, attrs in g.out_edges(paperId, data=True): \n",
    "        if attrs.get('label') == 'was_written_by': \n",
    "          g2.add_node(e2, label='author')\n",
    "          g2.add_edge(e1, e2, label='was_written_by')\n",
    "          g2.add_edge(e2, e1, label='wrote')\n",
    "        elif attrs.get('label') == 'cites' and e2 in inf_papers: \n",
    "          if e2 not in g2.nodes: \n",
    "            nAuthors = g.nodes[e2].get('nAuthors')\n",
    "            nRefs = g.nodes[e2].get('nRefs')\n",
    "            g2.add_node(e2, label='paper', nAuthors=nAuthors, nRefs=nRefs)\n",
    "          isInf = attrs.get('isInfluential')\n",
    "          g2.add_edge(e1, e2, label='cites', isInfluential=isInf)\n",
    "    return g2\n",
    "\n",
    "  # ~~~~~~~~~~ AUTHOR-INFLUENCE-GRAPH FUNCTIONS ~~~~~~~~~~  \n",
    "\n",
    "  def threshold_authors_by_pubcount(self, min_pub_count):\n",
    "    authors = sorted([int(a) for a,attrs in self.g.nodes.data() if attrs.get('label')=='author'])\n",
    "    authors_to_id = {a:i for i,a in enumerate(authors)}\n",
    "\n",
    "\n",
    "    n_authors = len(authors)\n",
    "    thresholded_authors = []\n",
    "    counts = []\n",
    "    for i,a in tqdm(enumerate(sorted(authors)), total=n_authors):\n",
    "      n_authors_articles = len([(e1,e2) for e1,e2 in self.g.out_edges(str(a)) if self.g.edges[e1,e2].get('label')=='wrote'])\n",
    "      counts.append(n_authors_articles)\n",
    "      if n_authors_articles > min_pub_count:\n",
    "        thresholded_authors.append(a)\n",
    "    return thresholded_authors, counts\n",
    "  \n",
    "  def compute_author_eigenfactors(self, thresholded_authors, alpha=0.99, verbose=False):\n",
    "    thresholded_authors_df = pd.DataFrame(thresholded_authors, columns=['id'])\n",
    "    n_thresholded_authors = len(thresholded_authors)\n",
    "    thresholded_authors_to_id = {a:i for i,a in enumerate(thresholded_authors)}\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Computing Z for %d authors\"%(n_thresholded_authors))\n",
    "    Z = dok_matrix((n_thresholded_authors, n_thresholded_authors), dtype=np.float64)\n",
    "    for i, authorId in tqdm(enumerate(thresholded_authors), total=len(thresholded_authors)):\n",
    "      edgeMap = self.compute_edges_for_author(authorId)\n",
    "      for t in edgeMap.keys():  \n",
    "        if int(t) in thresholded_authors:\n",
    "          j = thresholded_authors_to_id[int(t)]\n",
    "          Z[i,j] = edgeMap[t]\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Computing M = Z / Z_colsum\")\n",
    "    Z_colsum = np.sum(Z, axis=0) \n",
    "    M = Z / Z_colsum  \n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Computing A = teleport probability\")\n",
    "    v = []\n",
    "    n_all_articles = len([nid for nid, attrs in self.g.nodes.data() if attrs.get('label')=='paper'])\n",
    "    for i,a in tqdm(enumerate(thresholded_authors), total=n_thresholded_authors):\n",
    "      n_authors_articles = len([(e1,e2) for e1,e2 in self.g.out_edges(str(a)) if self.g.edges[e1,e2].get('label')=='wrote'])\n",
    "      v.append(n_authors_articles / n_all_articles)\n",
    "    A = np.array([v]).T @ np.ones((1,n_thresholded_authors))\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Computing P = alpha * M + (1-alpha) * A (alpha=%f)\"%(alpha))\n",
    "    P = alpha * M + (1-alpha) * A\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Computing Eigenfactors + Eigenvectors\")\n",
    "    PP = P + P.T\n",
    "    eigf, eigv = linalg.eig(np.nan_to_num(PP))\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Done\")\n",
    "    \n",
    "    leading_eigenvector_index = np.argmax(eigf)\n",
    "    f = eigv[:,leading_eigenvector_index].real#.reshape(P.shape[0],1)\n",
    "    thresholded_authors_df['f'] = f \n",
    "    \n",
    "    return thresholded_authors_df\n",
    "  \n",
    "  # modified from ./networkx/algorithms/traversal/breadth_first_search.py\n",
    "  def search_for_reference_author_pathways(self, source_author):\n",
    "    depth_limit = 3\n",
    "    out = []\n",
    "    queue = deque([('', source_author, depth_limit, self.g.successors(source_author))])\n",
    "    while queue:\n",
    "      route, parent, depth_now, children = queue[0]\n",
    "      for child in children:\n",
    "        if (self.g.nodes[child].get('label')=='paper' and depth_now>1) or (self.g.nodes[child].get('label')=='author' and depth_now==1):    \n",
    "          out.append(route+'|'+parent+'|'+child)\n",
    "          if depth_now > 1:\n",
    "            queue.append((route+'|'+parent, child, depth_now - 1, self.g.successors(child)))\n",
    "      queue.popleft()\n",
    "    return out\n",
    "\n",
    "\n",
    "  # modified from ./networkx/algorithms/traversal/breadth_first_search.py\n",
    "  def search_for_citation_author_pathways(self, source_author):\n",
    "    depth_limit = 3\n",
    "    out = []\n",
    "    queue = deque([('', source_author, depth_limit, self.g.predecessors(source_author))])\n",
    "    while queue:\n",
    "      route, parent, depth_now, children = queue[0]\n",
    "      for child in children:\n",
    "        if (self.g.nodes[child].get('label')=='paper' and depth_now>1) or (self.g.nodes[child].get('label')=='author' and depth_now==1):    \n",
    "          out.append(route+'|'+parent+'|'+child)\n",
    "          if depth_now > 1:\n",
    "            queue.append((route+'|'+parent, child, depth_now - 1, self.g.predecessors(child)))\n",
    "      queue.popleft()\n",
    "    return out\n",
    "\n",
    "  def compute_edges_for_author(self, a, forward=True):\n",
    "    weights = {}\n",
    "    if forward:\n",
    "      traversals = self.search_for_citation_author_pathways(str(a))\n",
    "    else:\n",
    "      traversals = self.search_for_reference_author_pathways(str(a))\n",
    "    for routes_string in traversals:\n",
    "      #routes_string = r+'|'+p+'|'+c\n",
    "      l = re.split('\\|', routes_string)\n",
    "      if len(l) == 5:\n",
    "        xxx, a1, p1, p2, a2 = l\n",
    "        p1_nAuthors = self.g.nodes[p1]['nAuthors'] if self.g.nodes[p1]['nAuthors']>0 else 5      \n",
    "        p1_nRefs = self.g.nodes[p1]['nRefs'] if self.g.nodes[p1]['nRefs']>0 else 100\n",
    "        p2_nAuthors = self.g.nodes[p2]['nAuthors'] if self.g.nodes[p2]['nAuthors']>0 else 100\n",
    "        x = 1 / (p1_nAuthors*p1_nRefs*p2_nAuthors) \n",
    "        a2 = int(l[4])\n",
    "        if weights.get(l[4]) is None:\n",
    "          weights[l[4]] = x\n",
    "        else: \n",
    "          weights[l[4]] += x\n",
    "    return weights\n",
    "  \n",
    "  \n",
    "  # ~~~~~~~~~~ UTILITIES ~~~~~~~~~~\n",
    "  def clone(self):\n",
    "    copy = KOLsOnS2AG(self.x_api_key)\n",
    "    copy.added_papers = self.added_papers\n",
    "    copy.g = self.g.copy()\n",
    "    copy.cit_g = self.cit_g.copy()\n",
    "    return copy\n",
    "    \n",
    "  def search_nodes(self, label):\n",
    "    nids = [(nid,attrs) for nid, attrs in self.g.nodes.data() if attrs.get('label')==label]\n",
    "    return nids\n",
    "\n",
    "  def search_edges(self, label):\n",
    "    edges = [(e1,e2,attrs) for e1,e2,attrs in self.g.edges.data() if attrs.get('label')==label]\n",
    "    return edges\n",
    "  \n",
    "  def load_from_pickle(self, file):\n",
    "    with open(file, 'rb') as f:\n",
    "      loaded_copy = pickle.load(f) \n",
    "    self.added_papers = loaded_copy.added_papers\n",
    "    self.g = loaded_copy.g\n",
    "\n",
    "  def save_to_pickle(self, file):\n",
    "    with open(file, 'wb') as f:\n",
    "      pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "  def check_before_adding_edges_from(self, edge_list, label):\n",
    "    checked_edge_list = [(e1,e2) for e1,e2 in edge_list if e1 in self.g.nodes and e2 in self.g.nodes]\n",
    "    if len(checked_edge_list) > 0: \n",
    "      print('adding %d new edges'%(len(checked_edge_list)))\n",
    "      self.g.add_edges_from(checked_edge_list, label=label)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(KOLsOnS2AG.print_basic_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(KOLsOnS2AG.search_for_disambiguated_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(KOLsOnS2AG.build_author_citation_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(KOLsOnS2AG.run_thresholded_centrality_analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
